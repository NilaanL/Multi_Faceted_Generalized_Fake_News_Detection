{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "!!! [21/02/2022] Exploring.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xQTJLpq0lHr8",
        "1BCmdGkWlLdL",
        "Q9gJ4AnhXej_",
        "9SNaP-1g0M1-",
        "jVqdjLOo1jxU"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries and Functions"
      ],
      "metadata": {
        "id": "xQTJLpq0lHr8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHXHzolR6OD0"
      },
      "outputs": [],
      "source": [
        "! pip install psutil -q\n",
        "! pip install pandarallel -q\n",
        "! pip install -U transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "workers = psutil.cpu_count()\n",
        "\n",
        "# from pandarallel.utils import progress_bars\n",
        "# progress_bars.is_notebook_lab = lambda : True\n",
        "\n",
        "from pandarallel import pandarallel\n",
        "pandarallel.initialize(progress_bar=True, nb_workers=workers, use_memory_fs=False)\n",
        "from transformers import AutoTokenizer, AutoModel, DistilBertTokenizerFast\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "tqdm.pandas()\n",
        "from tqdm import trange \n",
        "\n",
        "import torch, spacy, string, re, pickle\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "lWCZ5saz6fPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lower(text):\n",
        "  return text.lower()\n",
        "\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "def remove_nonascii(sent):\n",
        "  return \"\".join([i for i in sent if i.isascii()])\n",
        "\n",
        "def remove_punctuations(text):\n",
        "  res = re.sub(r'[^\\w\\s]', '', text)\n",
        "  return res\n",
        "\n",
        "def remove_num(text):\n",
        "  return \"\".join([c for c in text if not c.isdigit()])\n",
        "\n",
        "def remove_mul_space(text):\n",
        "  return \" \".join(text.split())\n",
        "\n",
        "def clean(text):\n",
        "  \n",
        "  text = lower(text)\n",
        "  text = remove_urls(text)\n",
        "  text = remove_nonascii(text)\n",
        "  # text = remove_punctuations(text)\n",
        "  # text = remove_num(text)\n",
        "  text = remove_mul_space(text)\n",
        "\n",
        "  return text\n",
        "\n",
        "def convert_label(label):\n",
        "  if label in ['true', 'mostly-true', 'half-true', 'real', 'Real', 0, 'REAL']:\n",
        "    return 0\n",
        "  if label in ['false', 'pants-fire', 'barely-true', 'fake', 'Fake', 1, 'FAKE']:\n",
        "    return 1\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "# Load model from HuggingFace Hub\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "# model = AutoModel.from_pretrained(r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Kogul_Language_Modelling/Fine tuning WELFake/Fine-tuned Model Improved\")\n",
        "model = AutoModel.from_pretrained(r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Kogul_Language_Modelling/Fine tuning WELFake/Fine-tuned Model Improved V3\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Generate Embeddings - This function returns the embeddings of all the texts\n",
        "def generate_embeddings(sentences, model, tokenizer):\n",
        "\n",
        "  encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt').to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    model_output = model(**encoded_input)\n",
        "\n",
        "  sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "\n",
        "  return sentence_embeddings.detach().cpu().numpy().tolist()"
      ],
      "metadata": {
        "id": "aseXrjOa6jA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Lexicon"
      ],
      "metadata": {
        "id": "1BCmdGkWlLdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-----------------------\")\n",
        "print(\"| Reading the Lexicon |\")\n",
        "print(\"-----------------------\")\n",
        "print()\n",
        "\n",
        "WELFAKE_MONOGRAMS = pd.read_csv(r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Lexicons/WELFake_Lexicon_Monograms.csv\")\n",
        "WELFAKE_BIGRAMS_TRIGRAMS_WITH_STOP = pd.read_csv(r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Lexicons/WELFake_Lexicon_Bigrams_Trigrams_With_Stop.csv\")\n",
        "WELFAKE_BIGRAMS_TRIGRAMS_WITHOUT_STOP = pd.read_csv(r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Lexicons/WELFake_Lexicon_Bigrams_Trigrams_Without_Stop.csv\")\n",
        "\n",
        "COLING_MONGRAMS = pd.read_csv(r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Lexicons/COLING_Lexicon_Monograms.csv\")\n",
        "COLING_BIGRAMS_TRIGRAMS_WITH_STOP = pd.read_csv(r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Lexicons/COLING_Lexicon_Bigrams_Trigrams_With_Stop.csv\")\n",
        "COLING_BIGRAMS_TRIGRAMS_WITHOUT_STOP = pd.read_csv(r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Lexicons/COLING_Lexicon_Bigrams_Trigrams_Without_Stop.csv\")\n",
        "\n",
        "lexicon = pd.concat([WELFAKE_MONOGRAMS, WELFAKE_BIGRAMS_TRIGRAMS_WITH_STOP], ignore_index=True)\n",
        "\n",
        "print(\"Done !!!\\n\")\n",
        "\n",
        "print(\"-----------------------\")\n",
        "print(\"| Creating Embeddings |\")\n",
        "print(\"-----------------------\")\n",
        "print()\n",
        "\n",
        "EMBEDDING_RANGE = 300\n",
        "\n",
        "all_words = lexicon['word'].tolist()\n",
        "all_words_embeddings = []\n",
        "\n",
        "for i in trange(0, len(all_words), EMBEDDING_RANGE):\n",
        "  all_words_embeddings.extend(generate_embeddings(all_words[i:i+EMBEDDING_RANGE], model, tokenizer))\n",
        "\n",
        "lexicon['embedding'] = all_words_embeddings\n",
        "\n",
        "print(\"\\nDone !!!\\n\")"
      ],
      "metadata": {
        "id": "yet53yP_6mWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WELFAKE_MONOGRAMS.info()\n",
        "WELFAKE_BIGRAMS_TRIGRAMS_WITH_STOP.info()"
      ],
      "metadata": {
        "id": "9VOp3As5DsHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lexicon.info()"
      ],
      "metadata": {
        "id": "wec4EfmY0E2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def removing_outliers(lexicon, algorithm):\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "  import seaborn as sns\n",
        "  %matplotlib inline\n",
        "\n",
        "  from sklearn.decomposition import PCA\n",
        "\n",
        "  print(\"---------------------------\")\n",
        "  print(\"| Current Embedding Space |\")\n",
        "  print(\"---------------------------\")\n",
        "  print()\n",
        "\n",
        "  pca = PCA(n_components=2)\n",
        "  pca.fit(lexicon['embedding'].tolist())\n",
        "\n",
        "  x_pca = pca.transform(lexicon['embedding'].tolist())\n",
        "\n",
        "  plt.figure(figsize=(16,14))\n",
        "  plt.scatter(x_pca[:,0],x_pca[:,1])\n",
        "\n",
        "  plt.xlabel('First principal component')\n",
        "  plt.ylabel('Second Principal Component')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  print(\"Done !!!\\n\")\n",
        "\n",
        "  print(\"---------------------\")\n",
        "  print(\"| Removing Outliers |\")\n",
        "  print(\"---------------------\")\n",
        "  print()\n",
        "\n",
        "  data = lexicon['embedding'].tolist()\n",
        "\n",
        "  if algorithm == \"SVM\":\n",
        "    from sklearn.svm import OneClassSVM\n",
        "    svm = OneClassSVM(nu=0.01)\n",
        "    yhat = svm.fit_predict(data)\n",
        "\n",
        "  elif algorithm == \"IsolationForest\":\n",
        "    from sklearn.ensemble import IsolationForest\n",
        "    # rs=np.random.RandomState(0)\n",
        "    # iso = IsolationForest(max_samples=100,random_state=rs, contamination=.1) \n",
        "    iso = IsolationForest(contamination=0.1)\n",
        "    yhat = iso.fit_predict(data)\n",
        "\n",
        "  elif algorithm == \"LocalOutlierFactor\":\n",
        "    from sklearn.neighbors import LocalOutlierFactor\n",
        "    lof = LocalOutlierFactor()\n",
        "    yhat = lof.fit_predict(data)\n",
        "\n",
        "  elif algorithm == \"EllipticEnvelope\":\n",
        "    from sklearn.covariance import EllipticEnvelope\n",
        "    ee = EllipticEnvelope(contamination=0.01)\n",
        "    yhat = ee.fit_predict(data)\n",
        "  \n",
        "  else:\n",
        "    print(\"Algorithm not defined properly !!!\")\n",
        "    return \"Error\"\n",
        "\n",
        "\n",
        "  mask = yhat != -1\n",
        "\n",
        "  labels = []\n",
        "\n",
        "  for i in range(len(mask)):\n",
        "    if mask[i] == False:\n",
        "      labels.append(i)\n",
        "\n",
        "  df = lexicon.drop(labels=labels, axis=0)\n",
        "\n",
        "  df.reset_index(inplace=True)\n",
        "\n",
        "  print(\"Done !!!\\n\")\n",
        "\n",
        "  print(\"The number of words in the Lexicon is {}\".format(len(lexicon)))\n",
        "  print(\"The number of outliers in the Lexicon is {}\".format(len(labels)))\n",
        "  print(\"Final number of words in the Lexicon is {}\".format(len(df)))\n",
        "  print()\n",
        "\n",
        "  print(\"----------------------------\")\n",
        "  print(\"| Improved Embedding Space |\")\n",
        "  print(\"----------------------------\")\n",
        "  print()\n",
        "\n",
        "  pca = PCA(n_components=2)\n",
        "  pca.fit(df['embedding'].tolist())\n",
        "\n",
        "  x_pca = pca.transform(df['embedding'].tolist())\n",
        "\n",
        "  plt.figure(figsize=(16,14))\n",
        "  plt.scatter(x_pca[:,0],x_pca[:,1])\n",
        "\n",
        "  plt.xlabel('First principal component')\n",
        "  plt.ylabel('Second Principal Component')\n",
        "\n",
        "  print(\"Done !!!\\n\")\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "sQgtf-gbvxsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lexicon = removing_outliers(lexicon, \"IsolationForest\")"
      ],
      "metadata": {
        "id": "o62lyqbsZd8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"------------------------\")\n",
        "print(\"| Generate Annoy Graph |\")\n",
        "print(\"------------------------\")\n",
        "print()\n",
        "\n",
        "print(\"Installing Annoy\")\n",
        "print(\"================\")\n",
        "\n",
        "! pip install annoy -q\n",
        "from annoy import AnnoyIndex\n",
        "\n",
        "print(\"\\nDone !!!\\n\")\n",
        "\n",
        "print(\"Creating the Annoy graph\")\n",
        "print(\"========================\")\n",
        "\n",
        "word_embeddings = lexicon['embedding'].tolist()\n",
        "\n",
        "f = 768\n",
        "ann = AnnoyIndex(f, 'angular')  # Length of item vector that will be indexed\n",
        "for i in range(len(word_embeddings)):\n",
        "    ann.add_item(i, word_embeddings[i])\n",
        "\n",
        "status = ann.build(1000) # 1000 trees\n",
        "\n",
        "if (status == True):\n",
        "  print(\"\\nTree built successfully !!!\\n\")\n",
        "\n",
        "else:\n",
        "  print(\"\\nTree was not built, ERROR !!!\\n\")"
      ],
      "metadata": {
        "id": "hp1nCMp-GXDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_scores(embedding):\n",
        "\n",
        "  true, fake = 0, 0\n",
        "  total_fake_doc, total_fake_occ = 0, 0\n",
        "  total_true_doc, total_true_occ = 0, 0\n",
        "\n",
        "  indexes, distances = ann.get_nns_by_vector(embedding, 100, include_distances=True, search_k=-1)\n",
        "  \n",
        "  count_true, count_fake = 0, 0\n",
        "\n",
        "  for j in range(len(indexes)):\n",
        "\n",
        "    fake_doc, true_doc = lexicon['fake_doc_score'][indexes[j]], lexicon['true_doc_score'][indexes[j]]\n",
        "    fake_occ, true_occ = lexicon['fake_occ_score'][indexes[j]], lexicon['true_occ_score'][indexes[j]]\n",
        "    \n",
        "    fake_occ_this, true_occ_this = fake_occ * ((fake_occ) / (fake_occ + true_occ)), true_occ * ((true_occ) / (fake_occ + true_occ))\n",
        "    fake_doc_this, true_doc_this = fake_doc * ((fake_doc) / (fake_doc + true_doc)), true_doc * ((true_doc) / (fake_doc + true_doc))\n",
        "\n",
        "    total_fake_occ += fake_occ_this\n",
        "    total_fake_doc += fake_doc_this\n",
        "    \n",
        "    total_true_occ += true_occ_this\n",
        "    total_true_doc += true_doc_this\n",
        "    \n",
        "\n",
        "    if fake_doc_this != 0 and true_doc_this != 0:\n",
        "      fake += (fake_occ_this / fake_doc_this) / distances[j]\n",
        "      true += (true_occ_this / true_doc_this) / distances[j]\n",
        "\n",
        "    elif fake_doc_this == 0 :\n",
        "      count_true += 1\n",
        "      true += (true_occ_this / true_doc_this) / (distances[j])\n",
        "\n",
        "    else:\n",
        "      count_fake += 1\n",
        "      fake += (fake_occ_this / fake_doc_this) / (distances[j])\n",
        "\n",
        "  return [fake*100/(fake+true), true*100/(fake+true), total_fake_occ, total_fake_doc, total_true_occ, total_true_doc]"
      ],
      "metadata": {
        "id": "ebHpJkx3HOJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9gJ4AnhXej_"
      },
      "source": [
        "# Analysis using LIAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H57L4toJgHl"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Datasets/LIAR/Liar_all.csv\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOZJfTIuJgHm"
      },
      "outputs": [],
      "source": [
        "# try:\n",
        "#   df = df.loc[df['split'] == 'test']\n",
        "# except KeyError:\n",
        "#   pass\n",
        "\n",
        "df['cleaned_statement'] = df['statement'].apply(clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvMmEbFuJgHn"
      },
      "outputs": [],
      "source": [
        "# df = df.drop_duplicates(subset=[\"cleaned_statement\"]).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUxvpI9GbFpX"
      },
      "outputs": [],
      "source": [
        "all_words = df['cleaned_statement'].tolist()\n",
        "all_words_embeddings = []\n",
        "\n",
        "EMBEDDING_RANGE = 200\n",
        "\n",
        "for i in trange(0, len(all_words), EMBEDDING_RANGE):\n",
        "  all_words_embeddings.extend(generate_embeddings(all_words[i:i+EMBEDDING_RANGE], model, tokenizer))\n",
        "\n",
        "df['embedding'] = all_words_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"scores\"] = df[\"embedding\"].progress_apply(get_scores)"
      ],
      "metadata": {
        "id": "LG3quBlgdfyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/LIAR/Liar_with_WELFAKE_Lexicon_Scores_Modified.csv\")"
      ],
      "metadata": {
        "id": "kZ87TLhmd8Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "ZDTmW1e_y_6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SNaP-1g0M1-"
      },
      "source": [
        "# Analysis using CodaLab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWDeq67R0M2A"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Datasets/CodaLab Covid/Constraint_English_All.csv\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MFaYgBW0M2B"
      },
      "outputs": [],
      "source": [
        "# try:\n",
        "#   df = df.loc[df['split'] == 'test']\n",
        "# except KeyError:\n",
        "#   pass\n",
        "\n",
        "df['cleaned_statement'] = df['tweet'].apply(clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIb_3PSS0M2C"
      },
      "outputs": [],
      "source": [
        "# df = df.drop_duplicates(subset=[\"cleaned_statement\"]).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivBbBZiM0M2D"
      },
      "outputs": [],
      "source": [
        "all_words = df['cleaned_statement'].tolist()\n",
        "all_words_embeddings = []\n",
        "\n",
        "EMBEDDING_RANGE = 100\n",
        "\n",
        "for i in trange(0, len(all_words), EMBEDDING_RANGE):\n",
        "  all_words_embeddings.extend(generate_embeddings(all_words[i:i+EMBEDDING_RANGE], model, tokenizer))\n",
        "\n",
        "df['embedding'] = all_words_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"scores\"] = df[\"embedding\"].progress_apply(get_scores)"
      ],
      "metadata": {
        "id": "WjiKkCuS0M2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/CodaLab Covid/CodaLab_with_WELFAKE_Lexicon_Scores_Modified.csv\")"
      ],
      "metadata": {
        "id": "yzyIARvO0M2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "uKXm0x890M2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVqdjLOo1jxU"
      },
      "source": [
        "# Analysis using Kaggle RealFake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Q_ma82f1jxV"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Datasets/Kaggle_real_fake/fake_or_real_news.csv\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZr24HlX1jxW"
      },
      "outputs": [],
      "source": [
        "# try:\n",
        "#   df = df.loc[df['split'] == 'test']\n",
        "# except KeyError:\n",
        "#   pass\n",
        "\n",
        "df['total_text'] = df['title']+ ' ' + df['text']\n",
        "\n",
        "df['cleaned_statement'] = df['total_text'].apply(clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aG4-9qSr1jxW"
      },
      "outputs": [],
      "source": [
        "# df = df.drop_duplicates(subset=[\"cleaned_statement\"]).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Jal-DJK1jxW"
      },
      "outputs": [],
      "source": [
        "all_words = df['cleaned_statement'].tolist()\n",
        "all_words_embeddings = []\n",
        "\n",
        "EMBEDDING_RANGE = 200\n",
        "\n",
        "for i in trange(0, len(all_words), EMBEDDING_RANGE):\n",
        "  all_words_embeddings.extend(generate_embeddings(all_words[i:i+EMBEDDING_RANGE], model, tokenizer))\n",
        "\n",
        "df['embedding'] = all_words_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"scores\"] = df[\"embedding\"].progress_apply(get_scores)"
      ],
      "metadata": {
        "id": "tw8bofTG1jxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/Kaggle_real_fake/Kaggle_real_fake_with_WELFAKE_Lexicon_Scores_Modified.csv\")"
      ],
      "metadata": {
        "id": "CEY6hBuW1jxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "VfqJlTxq1jxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l3Gg17V7222"
      },
      "source": [
        "# Analysis using FakeNewsNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-oGQ5aG7223"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Datasets/FakeNewsNet/FakeNewsNet_All.csv\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCoGvKf17223"
      },
      "outputs": [],
      "source": [
        "# try:\n",
        "#   df = df.loc[df['split'] == 'test']\n",
        "# except KeyError:\n",
        "#   pass\n",
        "\n",
        "\n",
        "df['cleaned_statement'] = df['title'].apply(clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tflx0L7P7224"
      },
      "outputs": [],
      "source": [
        "# df = df.drop_duplicates(subset=[\"cleaned_statement\"]).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmhJzhLH7224"
      },
      "outputs": [],
      "source": [
        "all_words = df['cleaned_statement'].tolist()\n",
        "all_words_embeddings = []\n",
        "\n",
        "EMBEDDING_RANGE = 300\n",
        "\n",
        "for i in trange(0, len(all_words), EMBEDDING_RANGE):\n",
        "  all_words_embeddings.extend(generate_embeddings(all_words[i:i+EMBEDDING_RANGE], model, tokenizer))\n",
        "\n",
        "df['embedding'] = all_words_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"scores\"] = df[\"embedding\"].progress_apply(get_scores)"
      ],
      "metadata": {
        "id": "iQ3Z8hLW7224"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/FakeNewsNet/FakeNewsNet_with_WELFAKE_Lexicon_Scores_Modified.csv\")"
      ],
      "metadata": {
        "id": "YpIznc5w7224"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "R7B2Z3F67225"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR5oqFJ-GWtN"
      },
      "source": [
        "# Analysis using ISOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKmt5KGcGWtP"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Datasets/ISOT/ISOT.csv\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rf7lvP1qGWtR"
      },
      "outputs": [],
      "source": [
        "# try:\n",
        "#   df = df.loc[df['split'] == 'test']\n",
        "# except KeyError:\n",
        "#   pass\n",
        "\n",
        "df['total_text'] = df['title']+ ' ' + df['text']\n",
        "\n",
        "df['cleaned_statement'] = df['total_text'].apply(clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9s8AU3gzGWtS"
      },
      "outputs": [],
      "source": [
        "# df = df.drop_duplicates(subset=[\"cleaned_statement\"]).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPanYB8yGWtS"
      },
      "outputs": [],
      "source": [
        "all_words = df['cleaned_statement'].tolist()\n",
        "all_words_embeddings = []\n",
        "\n",
        "EMBEDDING_RANGE = 150\n",
        "\n",
        "for i in trange(0, len(all_words), EMBEDDING_RANGE):\n",
        "  all_words_embeddings.extend(generate_embeddings(all_words[i:i+EMBEDDING_RANGE], model, tokenizer))\n",
        "\n",
        "df['embedding'] = all_words_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"scores\"] = df[\"embedding\"].progress_apply(get_scores)"
      ],
      "metadata": {
        "id": "dXC91bBqGWta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/ISOT/ISOT_with_WELFAKE_Lexicon_Scores_Modified.csv\")"
      ],
      "metadata": {
        "id": "pfdU5K7qGWtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "YzK69SReGWtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "vkwiul-iUVeO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}