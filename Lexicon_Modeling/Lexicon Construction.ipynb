{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "!!! [21/02/2022] Creating the Lexicon.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "JdoTT8ZMKkFw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "A2fTzOPKxD83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo0Ai9oE4FKl"
      },
      "source": [
        "! pip install psutil -q\n",
        "! pip install pandarallel -q\n",
        "! pip install -U sentence-transformers -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries\n",
        "\n",
        "* psutil and pandarallel for parallel processing a dataframe\n",
        "* tqdm for tracking processing\n",
        "* Sentence transformer for generating embeddings\n",
        "* Pandas and Numpy for dataframe processing"
      ],
      "metadata": {
        "id": "JdoTT8ZMKkFw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdbRZo4T4GL3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffae1e1f-ed10-47fa-d9f5-d687d42428bf"
      },
      "source": [
        "import psutil\n",
        "workers = psutil.cpu_count()\n",
        "\n",
        "# from pandarallel.utils import progress_bars\n",
        "# progress_bars.is_notebook_lab = lambda : True\n",
        "\n",
        "from pandarallel import pandarallel\n",
        "pandarallel.initialize(progress_bar=True, nb_workers=workers, use_memory_fs=False)\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "from sentence_transformers import SentenceTransformer ,  util\n",
        "import torch, spacy, string, re, pickle\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Pandarallel will run on 2 workers.\n",
            "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE_hPlHHtj94"
      },
      "source": [
        "# **Creating the Lexicon Using WELFake**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining cleaning/ preprocessing pipeline. "
      ],
      "metadata": {
        "id": "NT6JTHrbLL3S"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGLHh0xFJck5"
      },
      "source": [
        "def lower(text):\n",
        "  return text.lower()\n",
        "\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "def remove_nonascii(sent):\n",
        "  return \"\".join([i for i in sent if i.isascii()])\n",
        "\n",
        "def remove_punctuations(text):\n",
        "  res = re.sub(r'[^\\w\\s]', '', text)\n",
        "  return res\n",
        "\n",
        "def remove_num(text):\n",
        "  return \"\".join([c for c in text if not c.isdigit()])\n",
        "\n",
        "def remove_mul_space(text):\n",
        "  return \" \".join(text.split())\n",
        "\n",
        "def clean(text):\n",
        "  \n",
        "  text = lower(text)\n",
        "  text = remove_urls(text)\n",
        "  text = remove_nonascii(text)\n",
        "  text = remove_punctuations(text)\n",
        "  text = remove_num(text)\n",
        "  text = remove_mul_space(text)\n",
        "\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the WELFake dataset for lexicon creation"
      ],
      "metadata": {
        "id": "VfUuXnm8LijI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aww__Q1yJtFS"
      },
      "source": [
        "df = pd.read_csv(r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Datasets/WELFake/WELFake_Simplified.csv\")\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3eEcm3gDl-M"
      },
      "source": [
        "print(\"-----------------\")\n",
        "print(\"| Cleaning Data |\")\n",
        "print(\"-----------------\")\n",
        "print()\n",
        "\n",
        "df_fake = df[df['label'] == 0].copy(deep=True)\n",
        "df_true = df[df['label'] == 1].copy(deep=True)\n",
        "\n",
        "print(\"True News\")\n",
        "print(\"=========\")\n",
        "df_true['total_text'] = df_true['total_text'].parallel_apply(clean)\n",
        "print()\n",
        "print(\"Fake News\")\n",
        "print(\"=========\")\n",
        "df_fake['total_text'] = df_fake['total_text'].parallel_apply(clean)\n",
        "print()\n",
        "\n",
        "print(\"Done !!!\\n\")\n",
        "\n",
        "print(\"--------------------\")\n",
        "print(\"| Lemmatizing Data |\")\n",
        "print(\"--------------------\")\n",
        "print()\n",
        "\n",
        "nlp_lemmatize = spacy.load(\"en\", disable = ['parser', 'ner', 'tagger', 'textcat'])\n",
        "\n",
        "print(\"True News\")\n",
        "print(\"=========\")\n",
        "df_true[\"total_text\"] = df_true[\"total_text\"].parallel_apply(lambda row: \" \".join([w.lemma_ for w in nlp_lemmatize(row)]))\n",
        "print()\n",
        "print(\"Fake News\")\n",
        "print(\"=========\")\n",
        "df_fake[\"total_text\"] = df_fake[\"total_text\"].parallel_apply(lambda row: \" \".join([w.lemma_ for w in nlp_lemmatize(row)]))\n",
        "print()\n",
        "\n",
        "print(\"Done !!!\\n\")\n",
        "\n",
        "true_data = df_true['total_text'].tolist()\n",
        "fake_data = df_fake['total_text'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Total number of true articles is {}'.format(len(true_data)))\n",
        "print('Total number of fake articles is {}'.format(len(fake_data)))"
      ],
      "metadata": {
        "id": "Ik4jChHkmeXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_monograms, true_bigrams, true_trigrams = 0, 0, 0\n",
        "fake_monograms, fake_bigrams, fake_trigrams = 0, 0, 0\n",
        "\n",
        "for data in fake_data:\n",
        "  words = len(data.split())\n",
        "  fake_monograms += words\n",
        "  fake_bigrams += words - 1\n",
        "  fake_trigrams += words - 2\n",
        "\n",
        "for data in true_data:\n",
        "  words = len(data.split())\n",
        "  true_monograms += words\n",
        "  true_bigrams += words - 1\n",
        "  true_trigrams += words - 2"
      ],
      "metadata": {
        "id": "ze6loBwRpRm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generateLexicon(fake_data, true_data, max_length, common, grams):\n",
        "\n",
        "  print(\"---------------------\")\n",
        "  print(\"| Configuring SpaCy |\")\n",
        "  print(\"---------------------\")\n",
        "  print()\n",
        "  \n",
        "  try:\n",
        "\n",
        "    import spacy\n",
        "    nlp = spacy.load(\"en\", disable = ['parser','ner', 'lemmatizer', 'textcat']) \n",
        "    nlp.max_length = max_length\n",
        "\n",
        "    print(\"Done !!!\\n\")\n",
        "\n",
        "    print(\"-------------------\")\n",
        "    print(\"| Generating Data |\")\n",
        "    print(\"-------------------\")\n",
        "    print()\n",
        "\n",
        "    from tqdm import trange, tqdm\n",
        "\n",
        "    stop = []\n",
        "    with open(\"/content/drive/Shareddrives/FYP - knk/Resources/SMART_STOP_WORDS.txt\", \"r\") as f:\n",
        "      for word in f:\n",
        "          # Here we remove the apostrophe as well\n",
        "          stop.append(word.strip().replace(\"'\",\"\"))\n",
        "\n",
        "    for i in trange(len(fake_data)):\n",
        "      tokens = fake_data[i].split()\n",
        "      tokens = [word for word in tokens if not word in stop]\n",
        "      fake_data[i] = \" \".join(tokens)\n",
        "\n",
        "    for i in trange(len(true_data)):\n",
        "      tokens = true_data[i].split()\n",
        "      tokens = [word for word in tokens if not word in stop]\n",
        "      true_data[i] = \" \".join(tokens)\n",
        "\n",
        "    print(\"\\nDone !!!\\n\")\n",
        "\n",
        "    print(\"----------------------------------\")\n",
        "    print(\"| Filtering out the Proper Nouns |\")\n",
        "    print(\"----------------------------------\")\n",
        "    print()\n",
        "\n",
        "    fake_words, fake_sentences = [], []\n",
        "    for doc in tqdm(nlp.pipe(fake_data, n_process=2, disable=[\"tok2vec\", \"parser\", \"ner\", \"tetcat\", \"attribute_ruler\", \"lemmatizer\"]), total=len(fake_data)):\n",
        "      sentence = []\n",
        "      for token in doc:\n",
        "        if(token.tag_ not in ['NNP', 'NNPS']):\n",
        "          fake_words.append(str(token))\n",
        "          sentence.append(str(token))\n",
        "      fake_sentences.append(\" \".join(sentence))\n",
        "\n",
        "    true_words, true_sentences = [], []\n",
        "    for doc in tqdm(nlp.pipe(true_data, n_process=2, disable=[\"tok2vec\", \"parser\", \"ner\", \"tetcat\", \"attribute_ruler\", \"lemmatizer\"]), total=len(true_data)):\n",
        "      sentence = []\n",
        "      for token in doc:\n",
        "        if(token.tag_ not in ['NNP', 'NNPS']):\n",
        "          true_words.append(str(token))\n",
        "          sentence.append(str(token))\n",
        "      true_sentences.append(\" \".join(sentence))\n",
        "\n",
        "    print(\"\\nDone !!!\\n\")\n",
        "\n",
        "    from nltk.util import ngrams\n",
        "    from collections import Counter\n",
        "\n",
        "    word_lists = [fake_words, true_words]\n",
        "    dictionary = {}\n",
        "\n",
        "    print(\"----------------------------------\")\n",
        "    print(\"| Extracting and Scoring N-grams |\")\n",
        "    print(\"----------------------------------\")\n",
        "    print()\n",
        "\n",
        "    for k in grams:\n",
        "\n",
        "      if k == 1:\n",
        "\n",
        "        fake_length, true_length = fake_monograms, true_monograms\n",
        "\n",
        "        print(\"Monograms\")\n",
        "        print(\"=========\")\n",
        "        print()\n",
        "\n",
        "      elif k == 2:\n",
        "\n",
        "        fake_length, true_length = fake_bigrams, true_bigrams\n",
        "\n",
        "        print(\"Bigrams\")\n",
        "        print(\"=======\")\n",
        "        print()\n",
        "\n",
        "      elif k == 3:\n",
        "\n",
        "        fake_length, true_length = fake_trigrams, true_trigrams\n",
        "\n",
        "        print(\"Trigrams\")\n",
        "        print(\"========\")\n",
        "        print()\n",
        "\n",
        "      for j in range (len(word_lists)):\n",
        "\n",
        "        n_grams = ngrams(word_lists[j], k)\n",
        "        c = Counter(n_grams).most_common(common)\n",
        "\n",
        "        for i in trange(len(c)):\n",
        "          item = c[i]\n",
        "          if k != 1:\n",
        "            b = \" \".join(item[0])\n",
        "          else:\n",
        "            b = item[0][0]\n",
        "\n",
        "          if b not in dictionary.keys():\n",
        "            dictionary[b] = {}\n",
        "          \n",
        "\n",
        "          if j == 0:\n",
        "            dictionary[b]['fake_occ_score'] = item[1] / float(fake_length)\n",
        "            doc_score = 0\n",
        "            for sent in fake_sentences:\n",
        "              if b in sent: \n",
        "                doc_score += 1\n",
        "            dictionary[b]['fake_doc_score'] = doc_score / float(len(fake_data))\n",
        "\n",
        "          else:\n",
        "            dictionary[b]['true_occ_score'] = item[1] / float(true_length)\n",
        "            doc_score = 0\n",
        "            for sent in true_sentences:\n",
        "              if b in sent: \n",
        "                doc_score += 1\n",
        "            dictionary[b]['true_doc_score'] = doc_score / float(len(true_data))\n",
        "\n",
        "      if '-PRON-' in dictionary.keys():\n",
        "        del dictionary['-PRON-']\n",
        "          \n",
        "      print(\"\\nDone !!!\\n\")\n",
        "\n",
        "    print(\"----------------------\")\n",
        "    print(\"| Creating Dataframe |\")\n",
        "    print(\"----------------------\")\n",
        "    print()\n",
        "\n",
        "    import pandas as pd\n",
        "    df = pd.DataFrame()\n",
        "    words_final, fake_occ_scores, fake_doc_scores, true_occ_scores, true_doc_scores = [], [], [], [] ,[]\n",
        "    for key in dictionary.keys():\n",
        "      words_final.append(key)\n",
        "      \n",
        "      try:\n",
        "        fake_occ_scores.append(dictionary[key]['fake_occ_score'])\n",
        "      except KeyError:\n",
        "        fake_occ_scores.append(0)\n",
        "      try:\n",
        "        fake_doc_scores.append(dictionary[key]['fake_doc_score'])\n",
        "      except KeyError:\n",
        "        fake_doc_scores.append(0)\n",
        "      try:\n",
        "        true_occ_scores.append(dictionary[key]['true_occ_score'])\n",
        "      except KeyError:\n",
        "        true_occ_scores.append(0)\n",
        "      try:\n",
        "        true_doc_scores.append(dictionary[key]['true_doc_score'])\n",
        "      except KeyError:\n",
        "        true_doc_scores.append(0)\n",
        "    \n",
        "    df['word'] = words_final\n",
        "    df['true_occ_score'], df['true_doc_score'] = true_occ_scores, true_doc_scores\n",
        "    df['fake_occ_score'], df['fake_doc_score'] = fake_occ_scores, fake_doc_scores\n",
        "\n",
        "    print(\"\\nDone !!!\\n\")\n",
        "\n",
        "    print(\"-------------------\")\n",
        "    print(\"| Created Lexicon |\")\n",
        "    print(\"-------------------\")\n",
        "    print()\n",
        "\n",
        "    print(\"Details\")\n",
        "    print(\"========\")\n",
        "    print()\n",
        "    print(df.info())\n",
        "\n",
        "    print(\"\\nSample\")\n",
        "    print(\"=======\")\n",
        "    print()\n",
        "    print(df.head(10))\n",
        "\n",
        "    return df\n",
        "\n",
        "  except ImportError:\n",
        "\n",
        "    print(\"SpaCy is not installed/ cannot be found.\")"
      ],
      "metadata": {
        "id": "B5ZRrmYtEcYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_DOC_LEN_IN_CHAR = 1000000\n",
        "MOST_COMMON_WORDS = 1000\n",
        "N_GRAMS = [1]\n",
        "\n",
        "lexicon = generateLexicon(fake_data, true_data, MAX_DOC_LEN_IN_CHAR, MOST_COMMON_WORDS, N_GRAMS)"
      ],
      "metadata": {
        "id": "zYODOZHFLKHS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}